{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43c32520",
   "metadata": {},
   "source": [
    "# 期末报告-探究多种ML方法在MNIST数据集上的分类和生成\n",
    "**数学与应用数学21-1 凌梓南 2021211589**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74de771e",
   "metadata": {},
   "source": [
    "## 1 摘要\n",
    "本实验报告旨在探究多种ML方法在手写数字数据集（MNIST）上的分类识别和生成的效果。报告主要分为两个部分，第一部分作者探究了分别使用Random Forest、SVM、MLP和CNN四种机器学习（深度学习）方法在MNIST数据集上对手写数字进行分类识别的实现与评估。第二部分中，作者基于Diffusion模型中的Ho等人提出的**DDPM**方法[[1]]，使用Peebles等人于2022年提出的**DiT**[[2]]作为骨干网络，借助MNIST数据集训练了一个Conditional Diffusion模型，实现可以通过输入数字来生成对应的手写数字（*这部分内容为作者正在参与的科研项目中的部分内容之一，为自主探索内容*）。</br>\n",
    "\n",
    "\n",
    "本篇报告的全部内容与源码作者已发布在GitHub个人仓库：\n",
    "\n",
    "[1]: https://arxiv.org/abs/2006.11239 \"Denoising Diffusion Probabilistic Models\"\n",
    "[2]: https://arxiv.org/abs/2212.09748 \"Scalable Diffusion Models with Transformers\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2638bca",
   "metadata": {},
   "source": [
    "## 2 MNIST数据集介绍\n",
    "\n",
    "MNIST是一个手写体数字的图片数据集，该数据集来由美国国家标准与技术研究所（National Institute of Standards and Technology (NIST)）发起整理，一共统计了来自250个不同的人手写数字图片，其中50%是高中生，50%来自人口普查局的工作人员。该数据集的收集目的是希望通过算法，实现对手写数字的识别。(本段介绍参考CSDN文章：[Mnist数据集简介](https://blog.csdn.net/tony_vip/article/details/118735261))\n",
    "\n",
    "图片来自维基百科：\n",
    "![image.png](img/MnistExamples.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eed86f6",
   "metadata": {},
   "source": [
    "## 3 MNIST手写数字数据集分类识别\n",
    "\n",
    "本节中，作者使用Random Forest, SVM, MLP和CNN四种方法对手写数字数据集进行分类识别，其中Random Forest和SVM方法使用`skit-learn`库进行实现，而MLP和CNN方法则使用Meta的`PyTorch`框架进行构造实现。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd4ddb7",
   "metadata": {},
   "source": [
    "### 3.1 MNIST数字识别-SVM实现\n",
    "\n",
    "在本小节中，作者使用使用`sklearn`库中的`datasets`模块直接读取内置在工具包中的MNIST数据集，然后进行8:2的训练集-测试集划分。通过`SVM`模型拟合labels，最终在测试集上借助`metrics`模块中的`classification_report`和混淆矩阵`confusion_matrix`评估结果，同时可视化了部分测试的结果。\n",
    "\n",
    "从结果中可以看出SVM方法在随机划分的测试集上取得了**94%**的准确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "96f6a1cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T12:46:46.330237Z",
     "start_time": "2024-04-30T12:46:46.138948Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.99        35\n",
      "           1       0.97      1.00      0.99        36\n",
      "           2       1.00      1.00      1.00        35\n",
      "           3       0.96      0.73      0.83        37\n",
      "           4       0.97      0.92      0.94        37\n",
      "           5       0.93      1.00      0.96        37\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       0.92      0.97      0.95        36\n",
      "           8       0.78      0.94      0.85        33\n",
      "           9       0.92      0.89      0.90        37\n",
      "\n",
      "    accuracy                           0.94       360\n",
      "   macro avg       0.94      0.94      0.94       360\n",
      "weighted avg       0.95      0.94      0.94       360\n",
      "\n",
      "Confusion Matrix:\n",
      "[[34  0  0  0  1  0  0  0  0  0]\n",
      " [ 0 36  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 35  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 27  0  2  0  2  6  0]\n",
      " [ 0  0  0  0 34  0  0  0  1  2]\n",
      " [ 0  0  0  0  0 37  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 37  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 35  1  0]\n",
      " [ 0  1  0  0  0  0  0  0 31  1]\n",
      " [ 0  0  0  1  0  1  0  1  1 33]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1600x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import datasets, svm, metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "X = digits.images\n",
    "y = digits.target\n",
    "\n",
    "n_samples = len(X)\n",
    "X = X.reshape((n_samples, -1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "\n",
    "svm_model = svm.SVC()\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "svm_predicted = svm_model.predict(X_test)\n",
    "\n",
    "#-----------------------验证阶段(直接写成函数方便后面RF复用)-----------------------#\n",
    "def test_report(predicted, visual=True):\n",
    "    print(\"Classification Report:\")\n",
    "    print(metrics.classification_report(y_test, predicted))\n",
    "\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(metrics.confusion_matrix(y_test, predicted))\n",
    "    \n",
    "    if visual:\n",
    "        _, axes = plt.subplots(nrows=1, ncols=4, figsize=(10, 3), dpi=160)\n",
    "        for ax, image, prediction in zip(axes, X_test, predicted):\n",
    "            ax.set_axis_off()\n",
    "            image = image.reshape(8, 8)\n",
    "            ax.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "            ax.set_title(f'Prediction: {prediction}')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "test_report(svm_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e48899",
   "metadata": {},
   "source": [
    "### 3.2 MNIST数字识别-Random Forest实现\n",
    "\n",
    "本小节中，作者更换骨干模型，使用随机森林作为分类器，数据和验证部分则复用上面SVM部分写好的代码。为了使报告简洁，作者在这一小节不再可视化最终的预测结果。\n",
    "\n",
    "经过测试发现，Random Forest在该数据集上的测试准确率为**93%**，不如SVM方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "973a5641",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T12:47:34.945310Z",
     "start_time": "2024-04-30T12:47:34.689601Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97        35\n",
      "           1       0.90      0.97      0.93        36\n",
      "           2       1.00      0.97      0.99        35\n",
      "           3       0.96      0.70      0.81        37\n",
      "           4       0.94      0.92      0.93        37\n",
      "           5       0.88      1.00      0.94        37\n",
      "           6       1.00      1.00      1.00        37\n",
      "           7       0.88      0.97      0.92        36\n",
      "           8       0.81      0.91      0.86        33\n",
      "           9       0.97      0.86      0.91        37\n",
      "\n",
      "    accuracy                           0.93       360\n",
      "   macro avg       0.93      0.93      0.93       360\n",
      "weighted avg       0.93      0.93      0.93       360\n",
      "\n",
      "Confusion Matrix:\n",
      "[[34  0  0  0  1  0  0  0  0  0]\n",
      " [ 0 35  0  0  0  0  0  0  0  1]\n",
      " [ 1  0 34  0  0  0  0  0  0  0]\n",
      " [ 0  1  0 26  0  3  0  1  6  0]\n",
      " [ 0  0  0  0 34  0  0  3  0  0]\n",
      " [ 0  0  0  0  0 37  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 37  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 35  1  0]\n",
      " [ 0  2  0  0  1  0  0  0 30  0]\n",
      " [ 0  1  0  1  0  2  0  1  0 32]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "\n",
    "rfc_model = RFC()\n",
    "rfc_model.fit(X_train, y_train)\n",
    "\n",
    "rfc_predicted = rfc_model.predict(X_test)\n",
    "\n",
    "test_report(rfc_predicted, visual=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcad434c",
   "metadata": {},
   "source": [
    "### 3.3 MNIST数字识别-MLP实现\n",
    "\n",
    "在本小节中，作者使用多层感知机(MLP)来学习MNIST数据集上的手写数字识别。算法模型实现使用Meta开发的深度学习框架`PyTorch`，数据集选择torch自带的MNIST数据集，在进行归一化处理后转换为tensor输入给构建的MLP模型。作者同样将train和test部分集成为函数，方便下一小节中的CNN进行复用。\n",
    "\n",
    "在MLP模型构建中，作者构建了一个三层MLP网络，其每层的size分别为：$[784, 128, 64, 10]$，每两个线性层之间使用$\\text{ReLU}(x) = \\max(0, x)$作为激活函数进行非线性化，同时加入了参数为0.05的正则化(Dropout)。损失函数选择内置了softmax的交叉熵损失函数`nn.CrossEntropyLoss()`，优化器选择为随机梯度下降(Stochastic Gradient Descent)`optim.SGD()`，学习率设置为0.01，并且进行了batch size为64的批量化分，累积所有批次的损失，然后统一进行小批量梯度下降更新参数。\n",
    "\n",
    "在10个epoch的训练后，最终在测试集上取得了**96.75%**的准确率，相比SVM和RF，提升较大。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d1cf65cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T13:25:45.980419Z",
     "start_time": "2024-04-30T13:25:45.920906Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "def train_model(model):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for images, labels in train_loader:\n",
    "            images = images.view(-1, 28*28)\n",
    "            labels = labels\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "def test_model(model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in test_loader:\n",
    "            images = images.view(-1, 28*28)\n",
    "            labels = labels\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Accuracy of the model on the test images: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "979cc2e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T13:29:05.783353Z",
     "start_time": "2024-04-30T13:27:48.477618Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 0.5686\n",
      "Epoch [2/20], Loss: 0.4700\n",
      "Epoch [3/20], Loss: 0.4793\n",
      "Epoch [4/20], Loss: 0.6113\n",
      "Epoch [5/20], Loss: 0.3293\n",
      "Epoch [6/20], Loss: 0.1983\n",
      "Epoch [7/20], Loss: 0.2042\n",
      "Epoch [8/20], Loss: 0.2712\n",
      "Epoch [9/20], Loss: 0.1898\n",
      "Epoch [10/20], Loss: 0.2913\n",
      "Epoch [11/20], Loss: 0.1609\n",
      "Epoch [12/20], Loss: 0.3141\n",
      "Epoch [13/20], Loss: 0.1432\n",
      "Epoch [14/20], Loss: 0.0566\n",
      "Epoch [15/20], Loss: 0.1736\n",
      "Epoch [16/20], Loss: 0.0860\n",
      "Epoch [17/20], Loss: 0.2021\n",
      "Epoch [18/20], Loss: 0.1215\n",
      "Epoch [19/20], Loss: 0.1296\n",
      "Epoch [20/20], Loss: 0.0818\n",
      "Accuracy of the model on the test images: 96.75%\n"
     ]
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self,input_size, hidden_sizes, output_size, dropout=0.05):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_sizes[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_sizes[1], output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "input_size = 784\n",
    "hidden_sizes = [128, 64]\n",
    "output_size = 10\n",
    "batch_size = 64\n",
    "learning_rate = 0.01\n",
    "epochs = 20\n",
    "\n",
    "mlp_model = MLP(input_size=input_size,\n",
    "               hidden_sizes=hidden_sizes,\n",
    "               output_size=output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(mlp_model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_model(mlp_model)\n",
    "test_model(mlp_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee1be34",
   "metadata": {},
   "source": [
    "### 3.4 MNIST数字识别-CNN实现\n",
    "\n",
    "在本节中，作者使用在CV领域中常用的深度学习方法卷积神经网络(Convolutional Neural Network, CNN)来进行手写数字数据集的识别分类。CNN中最为重要的就是卷积层，卷积层由可学习的卷积核组成，主要意义在于可以通过可学习的方式来自动实现对图片的特征提取，从而取代了传统机器学习研究中由领域专家进行人工手动特征工程的操作。1998年，Yan LeCun 等人发表了论文“Gradient-Based Learning Applied to Document Recognition”[[3]](https://ieeexplore.ieee.org/document/726791)，首次提出了基于卷积操作的LeNet-5网络，让人们得知了CNN网络的强大，而他们实验的数据集也正是MNIST。\n",
    "\n",
    "在本篇报告中，作者不再具体阐述CNN的具体工作原理，在GitHub仓库下，作者已将之前在其他课程作业中完成的**卷积神经网络结构与应用介绍**存放在`doc/卷积神经网络结构与应用介绍.pdf`中[<打开>](doc/卷积神经网络结构与应用介绍.pdf)，其中还介绍了作者使用PyTorch基于ResNet101迁移学习的情绪图片识别的项目案例。\n",
    "\n",
    "多通道卷积操作动图来自[Animated AI](https://animatedai.github.io/):\n",
    "![img.gif](img/cnn.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c86eef",
   "metadata": {},
   "source": [
    "在具体实现中，作者使用两层卷积核shape为$5 \\times 5$的卷积层，填充(Padding)为2，步长(Stride)为1，中间使用最大池化层(Max Pooling)，然后经过一个两个线性层输出结果。优化方面依旧选择交叉熵损失函数，SGD优化器，学习率为0.01，复用train和test函数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "77fa9dcc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T14:31:54.728955Z",
     "start_time": "2024-04-30T14:30:05.008495Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 0.3179\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [45]\u001b[0m, in \u001b[0;36m<cell line: 59>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m     57\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD(cnn_model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m---> 59\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcnn_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m test_model(cnn_model)\n",
      "Input \u001b[0;32mIn [45]\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m      5\u001b[0m images \u001b[38;5;241m=\u001b[39m images\n\u001b[1;32m      6\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\n\u001b[0;32m----> 8\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [45]\u001b[0m, in \u001b[0;36mCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 49\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# 展平操作\u001b[39;00m\n\u001b[1;32m     51\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_layer(x)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = 'cuda:0'\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv_layer = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), \n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Linear(64 * 7 * 7, 1000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1000, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layer(x)\n",
    "        x = x.view(x.size(0), -1) \n",
    "        x = self.fc_layer(x)\n",
    "        return x\n",
    "\n",
    "cnn_model = CNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(cnn_model.parameters(), lr=0.01)\n",
    "\n",
    "def train_model(model):\n",
    "    model.train()\n",
    "    for epoch in range(20):\n",
    "        for images, labels in train_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/20], Loss: {loss.item():.4f}')\n",
    "\n",
    "def test_model(model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Accuracy of the model on the test images: {100 * correct / total:.2f}%')\n",
    "\n",
    "train_model(cnn_model)\n",
    "test_model(cnn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025c542f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
